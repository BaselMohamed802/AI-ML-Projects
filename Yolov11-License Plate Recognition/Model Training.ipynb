{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719bd1f4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This project will be about making a license plate detection model using Yolov11. \n",
    "\n",
    "## The project goals are:\n",
    "1. Upload a car image from the front or back (where the license plate is).\n",
    "2. Segment/Extract the license plate from the image.\n",
    "3. Recognize and extract the text written using OCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6f8e1",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e4a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68258e91",
   "metadata": {},
   "source": [
    "# 2. Importing the Dataset\n",
    "Count the number of files in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6812ec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of files in data\\test\\images is 1020.\n",
      "The Number of files in data\\test\\labels is 1020.\n",
      "The Number of files in data\\train\\images is 7057.\n",
      "The Number of files in data\\train\\labels is 7057.\n",
      "The Number of files in data\\valid\\images is 2048.\n",
      "The Number of files in data\\valid\\labels is 2048.\n"
     ]
    }
   ],
   "source": [
    "# Function to count the images in a directory\n",
    "def get_directory(directory, structure=['images', 'labels']):\n",
    "    folders = os.listdir(directory)\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            subfolders = os.listdir(folder_path)\n",
    "            for final_folders in subfolders:\n",
    "                if final_folders in structure:\n",
    "                    final_folder_path = os.path.join(folder_path, final_folders)\n",
    "                    files = [f for f in os.listdir(final_folder_path) if os.path.isfile(os.path.join(final_folder_path, f))]\n",
    "                    num_files = len(files)\n",
    "                    print(f\"The Number of files in {final_folder_path} is {num_files}.\")\n",
    "\n",
    "# Print the number of files in each directory\n",
    "get_directory(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2edd5",
   "metadata": {},
   "source": [
    "# 3. Training the Model\n",
    "This section will be about training the model to segment the license plate from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf25292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.229 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=Yolov11-License-Plate-Model, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLO11s summary: 181 layers, 9,428,179 parameters, 9,428,163 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 2.50.7 MB/s, size: 19.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\data\\train\\labels... 7057 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 7057/7057 375.8it/s 18.8s<0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\data\\train\\labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 1.80.7 MB/s, size: 21.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\data\\valid\\labels... 2048 images, 3 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2048/2048 238.1it/s 8.6s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\data\\valid\\labels.cache\n",
      "Plotting labels to I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(73804bc09e944829a1cb80f78b81ff63) to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs\\mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mI:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/25      7.84G      1.299      1.712      1.196         36        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 6:60<2.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 1.5it/s 20.7s0.6s\n",
      "                   all       2048       2195       0.78      0.672      0.738      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/25       7.9G      1.291     0.8218      1.173         48        640: 100% ━━━━━━━━━━━━ 221/221 2.2s/it 8:12<1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.6it/s 12.2s0.4s\n",
      "                   all       2048       2195      0.856      0.764      0.821      0.531\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/25       7.9G      1.273     0.7649      1.169         32        640: 100% ━━━━━━━━━━━━ 221/221 1.7s/it 6:24<1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 1.9it/s 16.4s0.4s\n",
      "                   all       2048       2195      0.922      0.831      0.883      0.571\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/25      7.92G      1.249     0.7429      1.155         30        640: 100% ━━━━━━━━━━━━ 221/221 1.7s/it 6:17<1.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.6it/s 12.2s0.4s\n",
      "                   all       2048       2195       0.93      0.757      0.846      0.544\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/25       7.9G       1.22     0.7018      1.141         25        640: 100% ━━━━━━━━━━━━ 221/221 1.2s/it 4:16<0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 1.9it/s 16.5s0.4s\n",
      "                   all       2048       2195       0.92      0.886      0.926      0.599\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/25      7.92G      1.205     0.6655      1.129         32        640: 100% ━━━━━━━━━━━━ 221/221 2.0s/it 7:18<1.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.6it/s 12.5s0.4s\n",
      "                   all       2048       2195       0.94      0.877      0.935      0.622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/25       7.9G      1.181     0.6322      1.116         44        640: 100% ━━━━━━━━━━━━ 221/221 1.7s/it 6:24<1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 1.9it/s 16.9s0.4s\n",
      "                   all       2048       2195      0.971      0.899      0.944      0.627\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/25      7.92G      1.169     0.6163      1.111         29        640: 100% ━━━━━━━━━━━━ 221/221 1.5s/it 5:40<0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.7it/s 11.8s0.4s\n",
      "                   all       2048       2195       0.96      0.914      0.947      0.639\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/25       7.9G      1.156     0.6049      1.101         23        640: 100% ━━━━━━━━━━━━ 221/221 1.3s/it 4:37<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.1it/s 15.3s0.4s\n",
      "                   all       2048       2195      0.974      0.914      0.956      0.648\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/25      7.95G      1.149     0.5821      1.094         44        640: 100% ━━━━━━━━━━━━ 221/221 2.4s/it 8:48<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.5it/s 12.8s0.4s\n",
      "                   all       2048       2195      0.953      0.912      0.942      0.633\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/25       7.9G      1.127     0.5657      1.086         32        640: 100% ━━━━━━━━━━━━ 221/221 1.5s/it 5:39<1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 1.9it/s 16.4s0.4s\n",
      "                   all       2048       2195      0.969      0.923      0.957      0.651\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/25      7.92G      1.118     0.5563      1.083         34        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:44<1.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.5it/s 12.8s0.4s\n",
      "                   all       2048       2195      0.981      0.919      0.959      0.667\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/25       7.9G      1.119     0.5453      1.076         34        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 7:09<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 1.7it/s 19.1s0.4s\n",
      "                   all       2048       2195      0.988      0.921      0.961       0.67\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/25      7.92G      1.097     0.5236      1.066         35        640: 100% ━━━━━━━━━━━━ 221/221 1.6s/it 5:55<1.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.6it/s 12.4s0.4s\n",
      "                   all       2048       2195      0.985      0.928      0.964      0.686\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/25       7.9G       1.09     0.5216      1.063         32        640: 100% ━━━━━━━━━━━━ 221/221 1.1it/s 3:29<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.5it/s 12.9s0.4s\n",
      "                   all       2048       2195      0.981      0.922      0.957      0.667\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/25      7.92G      1.093     0.4914      1.088         17        640: 100% ━━━━━━━━━━━━ 221/221 1.2s/it 4:36<0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.8it/s 11.6s0.4s\n",
      "                   all       2048       2195      0.978      0.934      0.964      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/25       7.9G      1.077     0.4759      1.075         17        640: 100% ━━━━━━━━━━━━ 221/221 1.3it/s 2:50<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.8it/s 11.5s0.4s\n",
      "                   all       2048       2195      0.975      0.935      0.965      0.677\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 3 epochs. Best results observed at epoch 14, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=3) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "17 epochs completed in 1.770 hours.\n",
      "Optimizer stripped from I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\\weights\\last.pt, 19.2MB\n",
      "Optimizer stripped from I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\\weights\\best.pt, 19.2MB\n",
      "\n",
      "Validating I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\\weights\\best.pt...\n",
      "Ultralytics 8.3.228  Python-3.13.5 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 2080 SUPER, 8192MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 32/32 2.4it/s 13.2s0.4s\n",
      "                   all       2048       2195      0.985      0.928      0.964      0.685\n",
      "Speed: 0.1ms preprocess, 1.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mI:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "# Assigning the model\n",
    "model = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "# Train settings\n",
    "epochs = 25\n",
    "input_image_size = 640\n",
    "patience = 3\n",
    "batch = 32\n",
    "val = True\n",
    "cache = False\n",
    "plots = True\n",
    "project = \"Yolov11-License-Plate-Model\"\n",
    "workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Clear GPU Cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=\"data/data.yaml\",\n",
    "    epochs=epochs,\n",
    "    imgsz=input_image_size,\n",
    "    patience=patience,\n",
    "    batch=batch,\n",
    "    val=val,\n",
    "    cache=cache,\n",
    "    plots=plots,\n",
    "    project=project,\n",
    "    workers=workers,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbf4c8",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb161cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "model = YOLO(r\"I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\Yolov11-License-Plate-Model\\train5\\weights\\best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab2968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 License_Plate, 4.6ms\n",
      "1: 640x640 1 License_Plate, 4.6ms\n",
      "2: 640x640 1 License_Plate, 4.6ms\n",
      "3: 640x640 1 License_Plate, 4.6ms\n",
      "4: 640x640 1 License_Plate, 4.6ms\n",
      "5: 640x640 1 License_Plate, 4.6ms\n",
      "6: 640x640 1 License_Plate, 4.6ms\n",
      "7: 640x640 1 License_Plate, 4.6ms\n",
      "8: 640x640 1 License_Plate, 4.6ms\n",
      "9: 640x640 1 License_Plate, 4.6ms\n",
      "Speed: 1.7ms preprocess, 4.6ms inference, 1.9ms postprocess per image at shape (8, 3, 640, 640)\n",
      "Results saved to \u001b[1mI:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\inference results\\predict3\u001b[0m\n",
      "10 labels saved to I:\\AI-ML-Projects\\Yolov11-License Plate Recognition\\inference results\\predict3\\labels\n",
      "Results saved in directory inference results for 10 images\n"
     ]
    }
   ],
   "source": [
    "def read_random_images(folder_path, num_samples=10):\n",
    "    \"\"\"\n",
    "    Read random images from a folder\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images\n",
    "        num_samples (int): Number of images to read\n",
    "    \n",
    "    Returns:\n",
    "        list: List of PIL Image objects\n",
    "    \"\"\"\n",
    "    # Get all image files from the folder\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path) \n",
    "        if os.path.splitext(f)[1].lower() in image_extensions\n",
    "    ]\n",
    "    \n",
    "    # Check if we have enough images\n",
    "    if len(image_files) < num_samples:\n",
    "        raise ValueError(f\"Only {len(image_files)} images found, but {num_samples} requested\")\n",
    "    \n",
    "    # Randomly select images\n",
    "    selected_files = random.sample(image_files, num_samples)\n",
    "    \n",
    "    # Read and return images\n",
    "    images = []\n",
    "    for filename in selected_files:\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = Image.open(img_path)\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "# Load the test images\n",
    "test_dir_path = \"data/test/images\"\n",
    "test_images = read_random_images(test_dir_path, 10)\n",
    "\n",
    "# Perform inference on the test images\n",
    "def perform_inference(model, images_path, output_dir, batch_size=8):\n",
    "    \"\"\"\n",
    "    Function to perform inference on a list of images.\n",
    "\n",
    "    Args:\n",
    "        model (YoloModel): The YOLO model to use for inference.\n",
    "        images_path (list): A list of image file paths.\n",
    "        output_dir (str): The directory to save the output images.\n",
    "        batch_size (int): The batch size for inference.\n",
    "\n",
    "    Returns:\n",
    "        The results of the inference.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Perform inference\n",
    "    results = model(\n",
    "        images_path,\n",
    "        save=True,\n",
    "        save_txt=True,\n",
    "        save_conf=True,\n",
    "        project=output_dir,\n",
    "        batch=batch_size\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Perform inference on the test images\n",
    "output_inf_dir = \"inference results\"\n",
    "results = perform_inference(model, test_images, output_inf_dir, 8)\n",
    "print(f\"Results saved in directory {output_inf_dir} for {len(results)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450075cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82c51028",
   "metadata": {},
   "source": [
    "# 5. Extracting text from the license plates\n",
    "In this section, we will extract text from the license plate using python library easyocr, but first we have to preprocess the image to crop out the license plates detected from the image. So the project flow will be like:\n",
    "\n",
    "1. Model detected the license plate(s) from the image.\n",
    "2. Crop the license plate(s) from the image.\n",
    "3. Extract the text from the license plate(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82783393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the images\n",
    "def crop_license_plate(image):\n",
    "    # Read the image\n",
    "    img = cv2.imread(image)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
