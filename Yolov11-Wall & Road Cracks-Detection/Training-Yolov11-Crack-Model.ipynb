{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40c095c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In This file, I will train a Yolov11 model to detect wall cracks and road cracks, The project will use the Yolov11 segmentation large model version that is previously trained on the COCO dataset. However, I will retrain it on the custom dataset to detect wall and road cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a0aaa",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875d1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5dcac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42394e2",
   "metadata": {},
   "source": [
    "# 2. Importing the custom cracks dataset\n",
    "\n",
    "- Dataset Name: Crack Computer Vision Dataset\n",
    "- URL: https://universe.roboflow.com/apu-jimbr/crack-uuasm\n",
    "\n",
    "- Dataset Description: The dataset has about 4000 images of cracks in roads, walls, and cement in general.\n",
    "1. The Number of files in data\\test\\images is 322.\n",
    "2. The Number of files in data\\test\\labels is 322.\n",
    "3. The Number of files in data\\train\\images is 7072.\n",
    "4. The Number of files in data\\train\\labels is 7072.\n",
    "5. The Number of files in data\\valid\\images is 455.\n",
    "6. The Number of files in data\\valid\\labels is 455."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b685d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of files in data\\test\\images is 322.\n",
      "The Number of files in data\\test\\labels is 322.\n",
      "The Number of files in data\\train\\images is 14144.\n",
      "The Number of files in data\\train\\labels is 7072.\n",
      "The Number of files in data\\valid\\images is 910.\n",
      "The Number of files in data\\valid\\labels is 455.\n"
     ]
    }
   ],
   "source": [
    "def get_directory(directory, structure=['images', 'labels']):\n",
    "    folders = os.listdir(directory)\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            subfolders = os.listdir(folder_path)\n",
    "            for final_folders in subfolders:\n",
    "                if final_folders in structure:\n",
    "                    final_folder_path = os.path.join(folder_path, final_folders)\n",
    "                    files = [f for f in os.listdir(final_folder_path) if os.path.isfile(os.path.join(final_folder_path, f))]\n",
    "                    num_files = len(files)\n",
    "                    print(f\"The Number of files in {final_folder_path} is {num_files}.\")\n",
    "\n",
    "get_directory(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6cb6c9",
   "metadata": {},
   "source": [
    "# 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5aed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n-seg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be56b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Settings\n",
    "coco_path = \"data/data.yaml\"\n",
    "epochs = 25\n",
    "input_image_size = 640\n",
    "patience = 3\n",
    "batch = 32\n",
    "val = True\n",
    "cache = True\n",
    "cache= 'disk'\n",
    "plots = True\n",
    "project = \"Yolov11-test\"\n",
    "workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be210b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU Cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f1da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.228  Python-3.13.5 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 2080 SUPER, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=disk, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=3, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=Yolov11-test, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    683830  ultralytics.nn.modules.head.Segment          [2, 32, 64, [64, 128, 256]]   \n",
      "YOLO11n-seg summary: 203 layers, 2,842,998 parameters, 2,842,982 gradients, 9.7 GFLOPs\n",
      "\n",
      "Transferred 510/561 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 16.02.7 MB/s, size: 53.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\data\\train\\labels.cache... 7072 images, 461 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 7072/7072 6.8Mit/s 0.0s\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (8.1GB Disk): 100% ━━━━━━━━━━━━ 7072/7072 38.9Kit/s 0.2s.3s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 14.07.9 MB/s, size: 73.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\data\\valid\\labels.cache... 455 images, 3 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 455/455 551.1Kit/s 0.0s\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.5GB Disk): 100% ━━━━━━━━━━━━ 455/455 34.4Kit/s 0.0s\n",
      "Plotting labels to G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/18 16:30:36 INFO mlflow.tracking.fluent: Experiment with name 'Yolov11-test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(8c85230252444f74878c86f38ad62aac) to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs\\mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mG:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/25      5.62G      1.475      3.042      2.523      1.523         90        640: 100% ━━━━━━━━━━━━ 221/221 2.0s/it 7:31<1.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.1it/s 7.3s0.8ss\n",
      "                   all        455        780      0.203      0.258      0.106     0.0352       0.15      0.169     0.0468     0.0121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/25      5.13G       1.59      2.878      2.075      1.589         96        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:43<1.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.5it/s 5.2s0.8ss\n",
      "                   all        455        780       0.25      0.164      0.118     0.0288      0.182      0.169     0.0814     0.0168\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/25      5.23G      1.565      2.852      1.944      1.579         85        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:47<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.6it/s 5.0s0.7ss\n",
      "                   all        455        780      0.329      0.401       0.24     0.0787       0.25      0.257      0.136     0.0351\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/25      5.07G      1.536      2.817      1.883      1.564         85        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:48<1.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.6it/s 5.0s0.8ss\n",
      "                   all        455        780      0.322      0.334      0.233     0.0899      0.339      0.181      0.124     0.0348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/25      5.12G      1.457      2.779       1.78      1.512         95        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:43<1.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.8s0.7ss\n",
      "                   all        455        780      0.484      0.304      0.297      0.123      0.431      0.255      0.207     0.0637\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/25      5.12G        1.4      2.701      1.717      1.481         69        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:48<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.8s0.7ss\n",
      "                   all        455        780      0.527      0.286        0.3      0.137      0.418      0.214      0.187     0.0577\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/25      5.11G      1.363      2.646      1.652      1.454        101        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:43<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.6s0.7ss\n",
      "                   all        455        780      0.563      0.414      0.417      0.182      0.533      0.342      0.315      0.102\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/25       5.1G       1.32      2.623      1.575      1.425         95        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:39<1.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.6it/s 4.9s0.7ss\n",
      "                   all        455        780      0.639       0.45      0.494      0.235      0.542      0.376      0.347      0.127\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/25      5.07G      1.296      2.603       1.55      1.411         86        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 7:06<1.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.4s0.7ss\n",
      "                   all        455        780      0.437      0.408      0.368      0.176      0.416      0.314      0.262     0.0898\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/25      5.14G      1.263      2.533      1.506      1.388         86        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:43<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.7s0.7ss\n",
      "                   all        455        780      0.533      0.488      0.496      0.257      0.509      0.402      0.371      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/25       5.1G      1.255      2.501      1.475      1.381         74        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 6:55<0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.8s0.7ss\n",
      "                   all        455        780      0.527      0.443      0.443      0.237      0.486      0.388      0.337      0.125\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/25       5.1G      1.217      2.488      1.441      1.367        103        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:47<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.5s0.7ss\n",
      "                   all        455        780      0.576      0.474      0.504      0.273       0.54       0.43      0.413      0.164\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/25      5.23G      1.188      2.471      1.408      1.346         70        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:48<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.5s0.7ss\n",
      "                   all        455        780      0.434      0.404      0.398      0.206      0.499      0.273      0.261     0.0927\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/25      5.06G      1.191      2.435      1.405       1.35         88        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:29<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.4s0.7ss\n",
      "                   all        455        780      0.656      0.507      0.549       0.31      0.614      0.454      0.431      0.185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/25      5.29G      1.164      2.394      1.344      1.326         80        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:43<0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.6s0.7ss\n",
      "                   all        455        780       0.58      0.463      0.469      0.261      0.532      0.414      0.345      0.136\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/25      5.04G      1.222      2.125      1.464      1.542         61        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 6:49<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.4s0.7ss\n",
      "                   all        455        780      0.638      0.505       0.53      0.297      0.648      0.426      0.423      0.166\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/25      5.19G      1.176       2.08      1.397      1.496         38        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:35<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.8it/s 4.5s0.7ss\n",
      "                   all        455        780      0.671      0.558       0.58      0.327      0.587      0.488      0.457      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/25      5.23G      1.147      2.053       1.35      1.477         37        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:39<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.8s0.7ss\n",
      "                   all        455        780      0.722      0.479      0.561      0.315      0.682      0.409      0.439      0.172\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/25       5.2G      1.096      2.017       1.31      1.443         40        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 6:52<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.7it/s 4.8s0.7ss\n",
      "                   all        455        780      0.688      0.536      0.577      0.347       0.61      0.472      0.453      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/25      5.17G       1.07      1.987      1.265      1.427         39        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:44<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.5it/s 5.2s0.8ss\n",
      "                   all        455        780      0.636      0.525      0.573      0.336      0.602      0.472      0.466      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/25      5.04G      1.041      1.965      1.238      1.402         35        640: 100% ━━━━━━━━━━━━ 221/221 2.0s/it 7:24<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.6it/s 5.0s0.7ss\n",
      "                   all        455        780      0.675      0.544      0.588      0.348       0.66      0.436      0.458      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/25       5.2G      1.012      1.935      1.194      1.386         33        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 7:05<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.5it/s 5.2s0.8ss\n",
      "                   all        455        780      0.681      0.536      0.594       0.36      0.619      0.505      0.486      0.194\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/25      5.18G     0.9913      1.904      1.158      1.371         55        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:47<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.5it/s 5.3s0.8ss\n",
      "                   all        455        780      0.789      0.467      0.581      0.353      0.719      0.414      0.454      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/25       5.2G     0.9692      1.886      1.125      1.355         38        640: 100% ━━━━━━━━━━━━ 221/221 1.9s/it 6:55<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.5it/s 5.4s0.8ss\n",
      "                   all        455        780      0.646      0.525      0.575      0.357      0.658      0.427      0.453      0.195\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/25      5.02G     0.9398      1.854      1.107      1.338         40        640: 100% ━━━━━━━━━━━━ 221/221 1.8s/it 6:48<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.6it/s 5.1s0.8ss\n",
      "                   all        455        780      0.664      0.554      0.596       0.37      0.642      0.477      0.479      0.201\n",
      "\n",
      "25 epochs completed in 2.960 hours.\n",
      "Optimizer stripped from G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\\weights\\last.pt, 6.0MB\n",
      "Optimizer stripped from G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\\weights\\best.pt, 6.0MB\n",
      "\n",
      "Validating G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.228  Python-3.13.5 torch-2.8.0+cu129 CUDA:0 (NVIDIA GeForce RTX 2080 SUPER, 8192MiB)\n",
      "YOLO11n-seg summary (fused): 113 layers, 2,834,958 parameters, 0 gradients, 9.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 8/8 1.0it/s 7.9s0.9ss\n",
      "                   all        455        780      0.661      0.554      0.596       0.37      0.639      0.479       0.48      0.202\n",
      "                 crack        383        651      0.652      0.542      0.558      0.401      0.579      0.456      0.425       0.15\n",
      "              spalling        129        129      0.671      0.566      0.634      0.339      0.698      0.501      0.535      0.254\n",
      "Speed: 0.2ms preprocess, 1.7ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
      "Results saved to \u001b[1mG:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "results = model.train(data=coco_path, epochs=epochs, imgsz=input_image_size, patience=patience, batch=batch, val=val, cache=cache, plots=plots, project=project, workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2eb39",
   "metadata": {},
   "source": [
    "# 4. Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd7cd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f73f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all images present in the folder\n",
    "def read_images_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Read all images present in the folder\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            images.append(image)\n",
    "    return images\n",
    "\n",
    "# Inference Folder Path\n",
    "inference_folder_path = \"inference images\"\n",
    "\n",
    "# Read all images from the folder\n",
    "images = read_images_from_folder(inference_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04685c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 crack, 2.8ms\n",
      "1: 640x640 1 crack, 2.8ms\n",
      "2: 640x640 1 crack, 2.8ms\n",
      "3: 640x640 1 crack, 2.8ms\n",
      "4: 640x640 (no detections), 2.8ms\n",
      "5: 640x640 2 cracks, 2.8ms\n",
      "6: 640x640 1 crack, 2.8ms\n",
      "7: 640x640 2 cracks, 2.8ms\n",
      "8: 640x640 2 cracks, 2.8ms\n",
      "9: 640x640 1 crack, 2.8ms\n",
      "Speed: 1.9ms preprocess, 2.8ms inference, 1.5ms postprocess per image at shape (4, 3, 640, 640)\n",
      "Results saved to \u001b[1mG:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\inference images\\results\\predict\u001b[0m\n",
      "9 labels saved to G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\inference images\\results\\predict\\labels\n",
      "Results saved for 10 images\n"
     ]
    }
   ],
   "source": [
    "def batch_inference_save_results(model, image_paths, output_dir, batch_size=8):\n",
    "    \"\"\"\n",
    "    Perform batch inference and save results to files\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run inference and save results\n",
    "    results = model(image_paths, \n",
    "                   save=True,           \n",
    "                   save_txt=True,       \n",
    "                   save_conf=True,      \n",
    "                   project=output_dir,  \n",
    "                   batch=batch_size)    \n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load model\n",
    "model = YOLO(r\"G:\\Work Projects\\AI & ML Projects\\AI-ML-Projects\\Yolov11-Wall & Road Cracks-Detection\\Yolov11-test\\train\\weights\\best.pt\")\n",
    "\n",
    "# Run batch inference and save\n",
    "results = batch_inference_save_results(model, images, output_dir=\"inference images/results\", batch_size=4)\n",
    "print(f\"Results saved for {len(results)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8beeb2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
